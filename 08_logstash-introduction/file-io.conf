# bin/logstash -f conf.d/file-io.conf --config.test_and_exit
# bin/logstash -f conf.d/file-io.conf --config.reload.automatic

# write test logs (send to /tmp if you don't want to use sudo)
# open another terminal
#     sudo su -
#     touch /var/log/custom.log && chmod 0644 /var/log/custom.log
#     while true; do echo `date` >> /var/log/custom.log && sleep 5; done
# ctrl+c to exit the loop when finished

input {
	file {
		id => "file-input-varlog"
		# follow active files using tail mode
		#mode => "tail"
		mode => "read"
		path => "/var/log/custom.log"
		sincedb_path => "/tmp/sincedb"
		# consume "closed" files using read mode
		ignore_older => "10 weeks"
		# start_position is used with "tail" mode
		#start_position => "beginning"
	}
}

# if using json_lines as the output codec, you can remove the message or event.original fields, which are duplicates of each other
# this reduces output filesize by 50-75%
#filter {
#	mutate {
#		remove_field => [ "[message]" ]
#	}
#}

output {
	# this will slow down the pipeline considerably
	#stdout { codec => rubydebug }
	file {
		id => "file-output-tmp"
		path => "/tmp/output.log"
		# if using json_lines, remove [message] to reduce the output filesize by 50% or more
		# line adds the current timestamp, hostname, then original message; use sprintf to modify
		codec => line {
			format => "%{[message]}"
		}
	}
}
