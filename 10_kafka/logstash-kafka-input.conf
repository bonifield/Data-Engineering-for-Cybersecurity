# bin/logstash -f conf.d/logstash-kafka-input.conf --config.test_and_exit
# bin/logstash -f conf.d/logstash-kafka-input.conf --config.reload.automatic

# /tmp paths included for easier testing

input {
	kafka {
		id => "kafka-input"
		bootstrap_servers => "kafka01.local:9094,kafka02.local:9094"
		group_id => "logstash"
		security_protocol => "SSL"
		ssl_endpoint_identification_algorithm => "https"
		ssl_keystore_location => "/etc/ssl/elastic/logstash.local.flex.pkcs12"
		#ssl_keystore_location => "/tmp/wildcard.local.flex.pkcs12"
		ssl_keystore_password => "abcd1234"
		ssl_keystore_type => "PKCS12"
		ssl_key_password => "abcd1234"
		ssl_truststore_location => "/etc/ssl/elastic/truststore.jks"
		#ssl_truststore_location => "/tmp/truststore.jks"
		ssl_truststore_password => "abcd1234"
		ssl_truststore_type => "jks"
		topics => ["filebeat", "packetbeat", "winlogbeat", "rsyslog"]
		decorate_events => "extended"
	}
}

filter {
	# parse JSON if the event is coming from a Beat
	if [@metadata][kafka][topic] =~ /(?i)beat$/ {
		json {
			id => "json_parse_kafka_eo"
			source => "[event][original]"
		}
	}
	# remove event.original or message, depending on if you have any additional Elasticsearch ingest pipelines
	# or depending on what Elastic Agent integrations you have
	if [event][original] == [message] {
		mutate {
			remove_field => [ "[event][original]" ]
		}
	}
}

output {
	stdout { codec => rubydebug { metadata => true } }
}
